{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive dataset\n",
    "ds=pd.read_csv('nba_logreg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>18547</td>\n",
       "      <td>19118</td>\n",
       "      <td>18679</td>\n",
       "      <td>19812</td>\n",
       "      <td>0</td>\n",
       "      <td>1300</td>\n",
       "      <td>1170</td>\n",
       "      <td>0</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14479</td>\n",
       "      <td>10775</td>\n",
       "      <td>10998</td>\n",
       "      <td>12655</td>\n",
       "      <td>1300</td>\n",
       "      <td>3005</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>2000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>74770</td>\n",
       "      <td>75826</td>\n",
       "      <td>77741</td>\n",
       "      <td>79597</td>\n",
       "      <td>4000</td>\n",
       "      <td>3010</td>\n",
       "      <td>2600</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>29975</td>\n",
       "      <td>30559</td>\n",
       "      <td>29433</td>\n",
       "      <td>30045</td>\n",
       "      <td>2449</td>\n",
       "      <td>1778</td>\n",
       "      <td>2382</td>\n",
       "      <td>0</td>\n",
       "      <td>1251</td>\n",
       "      <td>1051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>58083</td>\n",
       "      <td>88344</td>\n",
       "      <td>49443</td>\n",
       "      <td>23159</td>\n",
       "      <td>5712</td>\n",
       "      <td>3503</td>\n",
       "      <td>59000</td>\n",
       "      <td>1600</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1  X2  X3  X4  X5  X6  X7  X8  X9  X10  ...    X14    X15    X16  \\\n",
       "0   20000   2   2   2  53   4   4   3   3    2  ...  18547  19118  18679   \n",
       "1  100000   2   1   2  28   0   0   0   2    0  ...  14479  10775  10998   \n",
       "2  200000   2   2   1  35   0   0   0   0    0  ...  74770  75826  77741   \n",
       "3   50000   2   1   1  43   0   0   0   0    2  ...  29975  30559  29433   \n",
       "4  100000   2   6   1  29   0   0   0   0    0  ...  58083  88344  49443   \n",
       "\n",
       "     X17   X18   X19    X20   X21   X22   X23  \n",
       "0  19812     0  1300   1170     0  1600     0  \n",
       "1  12655  1300  3005      0   700  2000  1500  \n",
       "2  79597  4000  3010   2600  3000  3000  3000  \n",
       "3  30045  2449  1778   2382     0  1251  1051  \n",
       "4  23159  5712  3503  59000  1600  1000  1000  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seperate features and output\n",
    "X=ds.iloc[:,0:18]\n",
    "Y=ds.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "X=X.to_numpy()\n",
    "ma=np.amax(X,axis=0)\n",
    "X=np.divide(X,ma)\n",
    "X=pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute data among training set and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split (X, Y, test_size=0.4)\n",
    "# Convert sets from datafram to numpy vectors\n",
    "X_train=X_train.to_numpy().T\n",
    "X_test=X_test.to_numpy().T\n",
    "Y_train=Y_train.to_numpy()\n",
    "Y_test=Y_test.to_numpy()\n",
    "# convert 1D vector into 2D vector\n",
    "Y_test=Y_test.reshape((1,X_test.shape[1]))\n",
    "Y_train=Y_train.reshape((1,X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "def init_params(dims):\n",
    "    parameters={}\n",
    "    for i in range(1,len(dims)):\n",
    "        parameters['W'+str(i)]=np.random.rand(dims[i-1],dims[i])*0.01\n",
    "        parameters['b'+str(i)]=np.random.rand(dims[i],1)*0.01\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propogation\n",
    "def for_prop(parameters,dims,X):\n",
    "    A=X\n",
    "    caches=[]\n",
    "    for i in range(1,len(dims)-1):\n",
    "        W=parameters['W'+str(i)]\n",
    "        b=parameters['b'+str(i)]\n",
    "        Z=np.dot(W.T,A)+b\n",
    "        linear_cache=(A,W,b)\n",
    "        A=np.maximum(0,Z)\n",
    "        activation_cache=(Z)\n",
    "        cache=(linear_cache,activation_cache)\n",
    "        caches.append(cache)\n",
    "    # For last classifier layer\n",
    "    W=parameters['W'+str(len(dims)-1)]\n",
    "    b=parameters['b'+str(len(dims)-1)]\n",
    "    Z=np.dot(W.T,A)+b\n",
    "    linear_cache=(A,W,b)\n",
    "    A=1/(1+np.exp(-Z))\n",
    "    activation_cache=(Z)\n",
    "    cache=(linear_cache,activation_cache)\n",
    "    caches.append(cache)\n",
    "    return A,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost(A,Y):\n",
    "    m=Y.shape[1]\n",
    "    J= np.sum((Y_train * np.log(A)) + ((1 - Y_train) * (np.log(1 - A))))/(-m)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives of Relu Activation WRT cost function\n",
    "def relu_deri(dA_prev,drZ):\n",
    "    drZ[drZ<=0] = 0\n",
    "    drZ[drZ>0] = 1\n",
    "    return np.multiply(dA_prev,drZ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propogation\n",
    "def back_prop(A,caches,Y,dims):\n",
    "    grades={}\n",
    "    m=Y.shape[1]\n",
    "    A_prev=A\n",
    "    dA_prev=None\n",
    "    for i in reversed(range(len(dims)-1)):\n",
    "        linear_cache,activation_cache=caches[i]\n",
    "        if i == len(dims)-2:\n",
    "            dZ=A_prev-Y\n",
    "            grades[\"dW\"+str(i+1)]=(1/m) * np.dot(dZ,linear_cache[0].T)\n",
    "            grades[\"db\"+str(i+1)]=np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "            grades[\"db\"+str(i+1)]=grades[\"db\"+str(i+1)].reshape((dZ.shape[0],1))\n",
    "            dA_prev=np.dot(linear_cache[1],dZ)\n",
    "        else:\n",
    "            dZ=relu_deri(dA_prev,activation_cache)\n",
    "            grades[\"dW\"+str(i+1)]=(1/m) * np.dot(dZ,linear_cache[0].T)\n",
    "            grades[\"db\"+str(i+1)]=np.squeeze(np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "            grades[\"db\"+str(i+1)]=grades[\"db\"+str(i+1)].reshape((dZ.shape[0],1))\n",
    "            dA_prev=np.dot(linear_cache[1],dZ)\n",
    "    return grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param(dims,parameters,grades,learning_rate):\n",
    "    for i in range(1,len(dims)):\n",
    "        parameters[\"W\"+str(i)]=parameters[\"W\"+str(i)]-(learning_rate*grades[\"dW\"+str(i)].T)\n",
    "        parameters[\"b\"+str(i)]=parameters[\"b\"+str(i)]-(learning_rate*grades[\"db\"+str(i)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=X_train.shape[1] # Nos of expamle\n",
    "features=X_train.shape[0] #Features\n",
    "\n",
    "# initialize parameter\n",
    "dims=[features,1]\n",
    "parameters=init_params(dims)\n",
    "\n",
    "# Hyperparameters\n",
    "iteration=1000\n",
    "learning_rate=0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6977636224907671\n",
      "0.6252027197720322\n",
      "0.5836550628459994\n",
      "0.5592452462082868\n",
      "0.5444620748661323\n",
      "0.5352260948526977\n",
      "0.5292718694445211\n",
      "0.5253064962655433\n",
      "0.5225721262034176\n",
      "0.5206137309843136\n"
     ]
    }
   ],
   "source": [
    "# Training Begins\n",
    "\n",
    "for i in range(iteration):\n",
    "    # Implement forward propogation\n",
    "    A,caches=for_prop(parameters,dims,X_train)\n",
    "\n",
    "    # Implement Cost function\n",
    "    J=cost(A,Y_train)\n",
    "\n",
    "    # Implement Backward propogation\n",
    "    grades=back_prop(A,caches,Y_train,dims)\n",
    "\n",
    "    # Update Parameters\n",
    "    parameters = update_param(dims,parameters,grades,learning_rate)\n",
    "    \n",
    "    if i%100==0:\n",
    "        print(J)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
